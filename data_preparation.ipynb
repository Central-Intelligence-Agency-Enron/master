{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_preparation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMx+CnBP84CoiEM4jSWiPnu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KZo3Hme1BQKd","colab_type":"text"},"source":["- This notebooks will read all emails under csv format which are located in /content/My Drive/Colab Notebooks/s_user_csv and clean it.\n","- Processed data is saved and used in other notebooks."]},{"cell_type":"markdown","metadata":{"id":"eHc0ymEcVetP","colab_type":"text"},"source":["**Setup the google colab environment**"]},{"cell_type":"code","metadata":{"id":"J5g6tyT7UT2V","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","# This will prompt for authorization.\n","# authorization code: 4/OwErfUj6QceGXhIGx_RWv0MKclb9rilw8UsJnZqFbSez-QS8zQ399JU\n","drive.mount('/content/drive')\n","\n","!pip install PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"idC0gfngVmlE","colab_type":"text"},"source":["**Import the libraries**"]},{"cell_type":"code","metadata":{"id":"FbOkr8ZgVmP9","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from joblib import dump, load\n","import nltk\n","nltk.download('stopwords')\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xKEPS4-ZWy-H","colab_type":"text"},"source":["**Google drive access path**"]},{"cell_type":"code","metadata":{"id":"LxTC_V2jWyYn","colab_type":"code","colab":{}},"source":["csv_path = '/content/drive/My Drive/Colab Notebooks/s_user_csv/'\n","metadata_path = '/content/drive/My Drive/Colab Notebooks/output/'\n","#metadata_ents_path = '/content/drive/My Drive/Colab Notebooks/metadata_ents'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EYJvPL_gXOkH","colab_type":"text"},"source":["**Select the content of emails of all users**"]},{"cell_type":"code","metadata":{"id":"dN8jiXP6XJ0C","colab_type":"code","colab":{}},"source":["def read_all_emails(file_path):\n","    '''\n","    Go through all csv files in file_path, read the file content\n","    - Agr:\n","        @file_path (string): the location of csv files\n","    - Return:\n","        a list of string (list)\n","    '''\n","\n","    list_file = os.listdir(file_path)\n","    list_content = []\n","    for f_csv in list_file:\n","    if ('.csv' in f_csv):\n","        # print(f_csv)\n","        data = pd.read_csv(file_path + '/' + f_csv, encoding = \"ISO-8859-1\")\n","        for content in data['content'].values:\n","            if (content):\n","                list_content.append(content)\n","    return list_content\n","\n","list_content = read_all_emails(csv_path)\n","# save list_content on drive\n","dump(list_content, metadata_path + 'list_content.joblib')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKhZ1CI_Zf_L","colab_type":"text"},"source":["- Clean the mail content:\n","  - Remove the punctuation\n","  - Remove non alpha numberic letters\n","  - Do the lematization on the content\n","  - Remove the stop words\n","  - Replace the contractions in english"]},{"cell_type":"code","metadata":{"id":"c7x5-4DIZfiW","colab_type":"code","colab":{}},"source":["contractions = { \n","\"ain't\": \"am not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"i'd\": \"i would\",\n","\"i'll\": \"i will\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'll\": \"it will\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"needn't\": \"need not\",\n","\"oughtn't\": \"ought not\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"she'd\": \"she would\",\n","\"she'll\": \"she will\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"that'd\": \"that would\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there had\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'll\": \"they will\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'll\": \"we will\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"who'll\": \"who will\",\n","\"who's\": \"who is\",\n","\"won't\": \"will not\",\n","\"wouldn't\": \"would not\",\n","\"you'd\": \"you would\",\n","\"you'll\": \"you will\",\n","\"you're\": \"you are\"\n","}\n","\n","en_stop = stopwords.words('english')\n","# this list is update during tuning hyper parameters of model\n","stoplist = ['j', 'one','re', 'thank', 'thanks', 'enron' , 'http' \\\n","            , 'the', 'email', 'to', 'send', 'pm', 'subject', 'please' \\\n","            , 'thanks', 'a', 'cc', 'bcc', 'i','from', 'we', 'r', 's','make' \\\n","            , 'want','forward', 'would','u', 'be','could','this','nt', 'say', 'rb', 'o', 'wr', 'tx', 'with', 'fyi', 'bc', 'he','web', 'click' \\\n","           ,'font', \"br\", \"net\", \"images\", \"gif\", \"com\", \"href\", \"text\", \"jpg\", \"script\", \"clear\", \"td\", \"size\", \"tr\", \"face\", \"align\" \\\n","        , \"align\", \"class\" , \"color\", \"pt\", \"border\", \"com\", \"www\", \"htm\", \"html\", \"width\", \"link\", \"go\", \"pdf\",  \"news\"]\n","\n","en_stop  = en_stop + stoplist\n","\n","def clean_data(list_text):\n","    \n","    list_content_processed = [list(simple_preprocess(text)) for text in list_text]\n","    \n","    list_content_clean = []\n","    for sentence in list_content_processed:\n","        list_sent = []\n","        # iterate each sentence in the corpus\n","        for word in sentence:\n","            if (not word in en_stop):\n","            if (word in contractions):\n","                list_sent.append(contractions.get(word))\n","            else:\n","                list_sent.append(word)\n","        \n","        if (len(list_sent) > 0):\n","            list_content_clean.append(list_sent)\n","    return list_content_clean\n","\n","list_content_clean = clean_data(list_content) \n","\n","dump(list_content_clean, metadata_path + 'list_content_clean.joblib')"],"execution_count":null,"outputs":[]}]}