{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"find_documents_extract_entities.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLFvXN//quXK3cnEL2Fv/R"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TJI4-2vXkKmw","colab_type":"text"},"source":["- Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic,we can find the documents a given topic has contributed to the most and infer the topic by reading that document.\n","- The notebook also extracts the named entiies of person, organization and people in the relevant emails. It also exports the list of topics into csv files. For each topic, there is one csv file that contains dominant documents, the named entity of peron, the named entity of organization and the name of money hidden in the email content."]},{"cell_type":"markdown","metadata":{"id":"5fVhonUWkxpR","colab_type":"text"},"source":["**Setup the google colab environment**"]},{"cell_type":"code","metadata":{"id":"Jzzhc6xKj-0x","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","# This will prompt for authorization.\n","# authorization code: 4/OwErfUj6QceGXhIGx_RWv0MKclb9rilw8UsJnZqFbSez-QS8zQ399JU\n","drive.mount('/content/drive')\n","\n","!pip install PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","!python -m spacy download en_core_web_md"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2R6sWR02k5Oi","colab_type":"text"},"source":["**Import libararies**"]},{"cell_type":"code","metadata":{"id":"6VIFva_lk7Dx","colab_type":"code","colab":{}},"source":["from joblib import load, dump\n","import pandas as pd\n","import os\n","from spacy import displacy\n","from collections import Counter\n","#from en_core_web_sm import load\n","from spacy.tokens import Span\n","from spacy.pipeline import EntityRuler\n","import en_core_web_md\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4F4JFL3YpsL-","colab_type":"code","colab":{}},"source":["nlp = en_core_web_md.load()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ij5kL9SQlNPY","colab_type":"text"},"source":["**Google drive access path**"]},{"cell_type":"code","metadata":{"id":"wpbmzmV2lRxf","colab_type":"code","colab":{}},"source":["metadata_path = '/content/drive/My Drive/Colab Notebooks/output/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5tIQYQL4laAb","colab_type":"text"},"source":["**Load the model, content and corpus**"]},{"cell_type":"code","metadata":{"id":"ajbQNw3RlfXf","colab_type":"code","colab":{}},"source":["lda_model = load(metadata_path + 'lda_model.joblib')\n","corpus = load(metadata_path + 'corpus.joblib')\n","\n","topic_csv_fname = 'mail_topics.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBteI-uWpxut","colab_type":"code","colab":{}},"source":["def get_money_entity(doc):\n","    money_ents = Counter([ent.text for ent in doc.ents if ent.label_ == 'MONEY'])\n","    #return list(money_ents)\n","    return ' '.join(money_ents)\n","\n","def get_organization_entity(doc):\n","    org_ents = Counter([ent.text for ent in doc.ents if ent.label_ == 'ORG'])\n","    # return list(join(org_ents))\n","    return ' '.join(org_ents)\n","\n","def get_person_entity(doc):\n","    person_ents = Counter([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n","    return ' '.join(person_ents"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JtgHJ4bDm5vd","colab_type":"text"},"source":["**1) Export the topic list into csv files**"]},{"cell_type":"code","metadata":{"id":"H-N6GCV8nCNy","colab_type":"code","colab":{}},"source":["topics = lda_model.print_topics(50)\n","\n","df_topics = pd.DataFrame(data = {'key_words' : topics}\n","                         , index = range(len(topics)) )\n","\n","df_topics.to_csv(metadata_path + topic_csv_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jw3Heik-lB1B","colab_type":"text"},"source":["**2) Find dominant documents to each category**"]},{"cell_type":"code","metadata":{"id":"O9uwVWVylKof","colab_type":"code","colab":{}},"source":["# Assigns the topics to the documents in corpus\n","lda_corpus = lda_model[corpus]\n","# find the dominant topics\n","lda_corpus = [max(prob, key = lambda y : y[1]) for prob in lda_corpus ]\n","list_content_by_topic = [[] for i in range(50)]\n","# select the most relevant documents to the topic\n","for i, x in enumerate(lda_corpus):\n","    #print(x[0])\n","    list_content_by_topic[x[0]].append(list_content[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2iwG9aGBqApt","colab_type":"text"},"source":["**3) Extrac the named enitiy of person, organization and money in the relevant emails. Finally, save them all as csv files**"]},{"cell_type":"code","metadata":{"id":"lQ4ltA2YqPlf","colab_type":"code","colab":{}},"source":["for idx, content in enumerate(list_content_by_topic):\n","\n","    for text in content:\n","        doc = nlp(text)\n","        list_money_ents = get_money_entity(doc)\n","        list_person_ents = get_person_entity(doc)\n","        list_org_ents = get_organization_entity(doc)\n","\n","    df_content = pd.DataFrame(data = {'mail_content' : content \n","                                      , 'person_entity': list_person_ents\n","                                      , 'money_entity' : list_money_ents\n","                                      , 'organisation_entity' : list_org_ents}\n","                                       , index = range(len(content)))\n","    \n","    content_csv_fname = \"mail_content_by_topics_\" + str(idx) + \".csv\"\n","\n","    df_content.to_csv(metadata_path + content_csv_fname)"],"execution_count":null,"outputs":[]}]}