{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_creation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOB9peCuV9AwUKu7iS1y0hb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bAybIeBXIEs2","colab_type":"text"},"source":["- This notebooks creates the corpus from text created from data_preparation.ipynb\n","- The purpose is to find the similar parts in the massive content and group them into a same category. Several attemps with several text clustering tecniques such as K-means clustering (KM), Laten Semantic Index (LSI) and Latent Direchet Allocation (LDA), LDA is selected because of coherent topics given.\n","- Finally, the  categories are plotted on graph and exported into HTML file.\n","- The model performance is evaludated by Coherence score of topics \n"]},{"cell_type":"markdown","metadata":{"id":"sTEtQSBCKyqD","colab_type":"text"},"source":["**Setup the google colab environment**"]},{"cell_type":"code","metadata":{"id":"aisgloq9HzrY","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","# This will prompt for authorization.\n","# authorization code: 4/OwErfUj6QceGXhIGx_RWv0MKclb9rilw8UsJnZqFbSez-QS8zQ399JU\n","drive.mount('/content/drive')\n","\n","!pip install PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","!pip install gensim\n","\n","!pip install pyldavis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JZ22gBy7K8ci","colab_type":"text"},"source":["**Import libararies**"]},{"cell_type":"code","metadata":{"id":"O1mh3BdTLAoo","colab_type":"code","colab":{}},"source":["from gensim.utils import simple_preprocess\n","import gensim.corpora as corpora\n","from gensim.models import CoherenceModel\n","from gensim.corpora.dictionary import Dictionary\n","from gensim import models\n","\n","\n","from joblib import dump, load\n","\n","# Plotting tools\n","from pyLDAvis.gensim import prepare  \n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5voVKSUvLNaw","colab_type":"text"},"source":["**Google drive access path**"]},{"cell_type":"code","metadata":{"id":"h_3VxjdnLPNs","colab_type":"code","colab":{}},"source":["csv_path = '/content/drive/My Drive/Colab Notebooks/s_user_csv/'\n","metadata_path = '/content/drive/My Drive/Colab Notebooks/output/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8SwOUAyrL0nS","colab_type":"text"},"source":["- **Create dictionary based on the content**\n","- **Make the bag of word coprus from the dictionary**\n","- **Transform the corpus into matrix term document**\n","- **Save the dictionary , corpus, matrix term document and LDA model on the drive**"]},{"cell_type":"code","metadata":{"id":"33dZwx7ELnmA","colab_type":"code","colab":{}},"source":["def make_corpus(list_text):\n","    dictionary = corpora.Dictionary(list_text)\n","    corpus = [content_dictionary.doc2bow(text) for text in list_text]\n","    return dictionary, corpus\n","\n","def make_lda_mode(dictionay, corpus, n_topics):\n","    lda_model = models.LdaModel(corpus, id2word = dictionay, num_topics = n_topics, alpha = 'auto', eval_every = 5)\n","    topics = lda_model.print_topics(n_topics)\n","    return lda_model, topics\n","\n","# load list_text_clean from drive\n","list_content_clean = load(metadata + 'list_content_clean.joblib')\n","dictionary, corpus = make_corpus(list_content_clean)\n","lda_model, topics = make_lda_model(dictionary, corpus)\n","\n","# save\n","dump(dictionary, metadata_path + \"dictionary.joblib\")\n","dump(corpus, metadata_path + \"corpus.joblib\")\n","dump(lda_model, metadata_path + \"lda_model.joblib\")\n","\n","\n","for idx, topic in topics:\n","    print(\"topic: {}\\n {}\".format(idx, topic))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8yMjmDEeOLJ8","colab_type":"text"},"source":["**Visualize the topcis with their key words**"]},{"cell_type":"code","metadata":{"id":"ps8rerbyOLkO","colab_type":"code","colab":{}},"source":["# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = prepare(lda_model, corpus, dictionary)\n","# export the model into htlm\n","pyLDAvis.save_html(vis, metadata_path + 'lda_model_50.html')\n","vis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8DInZ4ZPb7g","colab_type":"text"},"source":["- Computer Perplexity and Coherence Score to know good model is.\n","- The Perplexity is a mesure how good model is, the less, the better.\n","- Coherence score mesures how coherent model is, the more , the better. "]},{"cell_type":"code","metadata":{"id":"z5_eaYJwQRO6","colab_type":"code","colab":{}},"source":["# Compute Coherence Score\n","coherence_lda = CoherenceModel(model = lda_model\n","                                     , corpus = corpus\n","                                     , texts = list_content_clean\n","                                     , dictionary = dictionary \n","                                     , coherence='c_v')\n","\n","coherence = coherence_lda.get_coherence()\n","\n","print('\\nCoherence Score: ', coherence_lda)\n","\n","print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpxSOuODYO7P","colab_type":"text"},"source":["**Visualization topic distribution**"]},{"cell_type":"code","metadata":{"id":"T0vz-T0kYVmg","colab_type":"code","colab":{}},"source":["y_axis = []\n","x_axis = []\n","for topic_id, dist in topics:\n","    x_axis.append(topic_id + 1)\n","    y_axis.append(dist)\n","width = 1 \n","plt.bar(x_axis, y_axis, width, align='center', color='r')\n","plt.xlabel('Topics')\n","plt.ylabel('Probability')\n","plt.title('Topic Distribution for doc')\n","plt.xticks(np.arange(2, len(x_axis), 2), rotation='vertical', fontsize=7)\n","plt.subplots_adjust(bottom=0.2)\n","plt.ylim([0, np.max(y_axis) + .01])\n","plt.xlim([0, len(x_axis) + 1])\n","plt.savefig(output_path)\n","plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRay8sYASKz9","colab_type":"text"},"source":["**Find similar documents to each category**"]},{"cell_type":"code","metadata":{"id":"x5ZhuBiESWYr","colab_type":"code","colab":{}},"source":["# Assigns the topics to the documents in corpus\n","lda_corpus = lda_model[corpus]\n","# find the dominant topics\n","lda_corpus = [max(prob, key = lambda y : y[1]) for prob in lda_corpus ]\n","list_content_by_topic = [[] for i in range(50)]\n","# select the most relevant documents to the topic\n","for i, x in enumerate(lda_corpus):\n","    #print(x[0])\n","    list_content_by_topic[x[0]].append(list_content[i])"],"execution_count":null,"outputs":[]}]}